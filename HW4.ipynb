{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milly-lauren/aiHW4/blob/hw4/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlXOuODmCPMs",
        "colab_type": "text"
      },
      "source": [
        "## General Concepts\n",
        "\n",
        "\n",
        "* **Artifical Intelligeince** is when a computer program does anything smart without human intervention\n",
        "*   **Machine Learning** is taking an input that has never been seen before, and making a useful prediction from it and then adjusting istelf without human intervention\n",
        "    - Input can be any form of data but in this class we focused mainly on images as input, such as the MNIST library\n",
        "*   **Deep Learning** is a subset of Machine Learning \n",
        "\n",
        "\n",
        "The main proccess for setting up a machine learning system that we utilized throughout this semeseter is as follows:\n",
        "1. Define the input data, both training and test, and preparing the data for the model.\n",
        "2. Define the model, specifcially the different layers.\n",
        "3. Define the loss function and the optimizer function\n",
        "4. Train the network\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The code snippet below shows an example of defining input data, and getting it ready for a model, which will be illustrated in the remaining code snippets on this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z74ZQP0aAcmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "f37d592d-5fce-43d8-fb01-bc0e7e66dcd1"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "# Break the data into train and test groups\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Preparing the images since they are black and white binary images\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur1FEweHCVNC",
        "colab_type": "text"
      },
      "source": [
        "## Building a Model\n",
        "\n",
        "Models are first trained on the data set. There are multiple ways to train a model, for more information on that please see the next section. During this class to build models we specifically used Keras and TensorFlow. Keras is now apart of TensorFlow. \n",
        "\n",
        "In this class we mainly focused on two differnet kinds of models:\n",
        "*   Regressional Model : predicts continous values\n",
        "*   Classification Model : predicts discrete values\n",
        "\n",
        "When building a model the kind of layers needs to be chosen. In this class we focused on Dense Layers, Long-Short Term Memory, and 2D Convolutional. \n",
        "\n",
        "One important note to make is that each layer has an activation function, but only the first layer must contain the input shape size.\n",
        "\n",
        "Activation Functions we focused on this semester:\n",
        "* Sigmoid\n",
        "* Relu\n",
        "* Softmax\n",
        "\n",
        "A model is created from multiple layers. Network architecture is more of an *ART* than science.\n",
        "\n",
        "A specifc model we worked with a lot duirng this semester was a CNN or Convolutional Nueral Network. A CNN model is structured a little differently.\n",
        "\n",
        "The input matrix is put through two set of 2D Convolutional modules, before being flattened and put through a fully connected nural network. The 2DConv layers user the input feature map and the convolutional filter to create an output feature map. Training a CNN does take a lot more time but you are able to extract more distinct features and generally have better classificaiton. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "To keep with the same example, we are building a classification model. One that will classify different MNIST fashion items. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww-Dqp3LCYUR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "dd193b5a-2d7c-403c-fc4e-826df6ad1e11"
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(2, activation = tf.nn.relu),\n",
        "    keras.layers.Dense(3, activation = tf.nn.relu),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p_brcIXCYiP",
        "colab_type": "text"
      },
      "source": [
        "## Compiling a Model\n",
        "\n",
        "After completing the structure of the networks for the model a loss function and an optimizer need to be defined. \n",
        "\n",
        "**Loss Function**\n",
        "* Measure the success of the task at hand, and it is the quanitity that will be minimized during run time.\n",
        "* The loss functions we learned in this class:\n",
        "    - Binary Crossentropy (For binary classification)\n",
        "    - Categorical Crossentropy (For multi-class classification)\n",
        "    - MSE\n",
        "* We really only focused on using Categorical Crossentropy due to the nature of the assignments, since we were classifying images to multiple classes\n",
        "\n",
        "**Optimizers** \n",
        "* Based on the loss function, the optimzer is how the network updates itself.\n",
        "* Implements Stochastic Gradient Descent\n",
        "* Gradient Descent : algorithm used to minimize loss, calculate the gradiant based on the weight and bias value = w*x + b\n",
        "* Learning Rate : gradient multiplier to determine next step\n",
        "* Hyperparameters are what are changed in order to tweak algorithms, and the learning rate is manipulated a lot in order to find the best rate for training\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "To continue the example, the code snippet below compiles the model, and defines the metric for analyzing outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URacVMyKCnYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mwG6hsqCnoN",
        "colab_type": "text"
      },
      "source": [
        "## Training a Model\n",
        "\n",
        "Setting up the training for a model, a few things need to be taken into account. To train a good model, the model needs to see a good variety of never before seen data. But what exactly is a good variety? There is too much of a good thing when it comes to training a model. \n",
        "\n",
        "When a model is too complex and trains too well on a training set, meaning it is too precies, it will fail to make good predicitons when new data is seen in the test set. This is called overfitting. The soluation to overfitting it to keep models simple. Training over a large number of epochs will make the neurons too specific. Overfitting can be thought of as training the model too much. \n",
        "\n",
        "On the other hand we have underfitting. This occurs when a model is not trained enough. This happens when the training set is not a good enough size or representation of the data as a whole. Or the number of layers or number of epochs is too low to get any kind of generalization of the data. Splitting the data properly into test and training sets combats underfitting. \n",
        "\n",
        "Using Tensorflow, training a model is quite simple. Using the fit() function trains the model on the specified data and validates it on the specified test data at the end of each epoch. \n",
        "\n",
        "A model should **never** be trained on testing data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHJmSEbXCp6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c7e12257-3bf6-4cfb-95ab-360c38739a61"
      },
      "source": [
        "# Train the model\n",
        "epochs = 5\n",
        "history = model.fit(train_images, \n",
        "                      train_labels, \n",
        "                      epochs=epochs,  \n",
        "                      validation_data=(test_images, test_labels))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 1.8297 - acc: 0.2419 - val_loss: 1.6462 - val_acc: 0.2847\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 1.5672 - acc: 0.3228 - val_loss: 1.5221 - val_acc: 0.3329\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 1.4840 - acc: 0.3529 - val_loss: 1.4754 - val_acc: 0.3495\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 1.4373 - acc: 0.3805 - val_loss: 1.4209 - val_acc: 0.4036\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 1.3815 - acc: 0.4182 - val_loss: 1.3643 - val_acc: 0.4233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmsweZwIeX7R",
        "colab_type": "text"
      },
      "source": [
        "*This ends the code for this example. The remaining code will be for another example.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6fkzdweCqE-",
        "colab_type": "text"
      },
      "source": [
        "## Fine tuning a pretrained model\n",
        "\n",
        "The major benefits of using pre-trained models is the data and computing time it saves us. To use a pretrained model how we want to, it is important to fine tune it. The following are steps used to fine tune a pretrained model, similarily to how homework 3 was completed:\n",
        "\n",
        "1. Freeze the layers that shoudl not be trained\n",
        "2. Define data generators for both the training and testing data sets\n",
        "3. Build a new model with the pretrained model, basically adding whatever layers you wish\n",
        "4. Train! (Slightly different than before using fit_generator())\n",
        "\n",
        "Layers can be unfrozen during training.\n",
        "\n",
        "\n",
        "---\n",
        "This code will not compile as it it just code snipets taken and modified from homework 3. This is just for example of the basics for fine-tuning a pretrained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdeHM24zCtNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze layers\n",
        "for layer in NASNetLarge[:-5]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Creating the new model using the pretrained model\n",
        "    conv_base1 = NASNetLarge(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "    model = models.Sequential()\n",
        "    model.add(conv_base1)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "# Example of creating a data generator\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    dataset_path+'test',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    classes=classnames,\n",
        "    class_mode='categorical')\n",
        "\n",
        "\n",
        "# Training the added layers only\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=learning_rate, decay=lr_decay), metrics=['acc'])\n",
        "\n",
        "callbacks_list_L1 = [ModelCheckpoint(filepath=weights_path, save_weights_only=True, monitor='val_acc', verbose=1, save_best_only=True),\n",
        "                      ReduceLROnPlateau(monitor='val_acc', factor=factorL1, patience=patiencel1, verbose=1),\n",
        "                      TensorBoard(log_dir=TensorBoardLogDir+'\\\\level1')]\n",
        "\n",
        "# Training the model\n",
        "history = model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=(nbrTrainImages * classes) // 100 * batch,\n",
        "        epochs=epochsL1,\n",
        "        callbacks=callbacks_list_L1,\n",
        "        validation_data=test_generator,\n",
        "        validation_steps=nbrTestImages,\n",
        "        verbose=verbose_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgLNUlhpdIwh",
        "colab_type": "text"
      },
      "source": [
        "I really enjoyed this class and learning about the basics of machine learning and nueral networks. My favorite homework assignment was the second homework assignment, specifically working with the MNIST Fashion items. That was a good challenge, but is something that was applicable to another project I was working on. I think CNN are the coolest, just because of the process they go through. Thanks for a great semester!!"
      ]
    }
  ]
}